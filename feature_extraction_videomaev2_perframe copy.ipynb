{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba9491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cair/miniconda3/envs/videomaev2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from VideoMAEv2.models.modeling_pretrain import pretrain_videomae_base_patch16_224 as VideoMAEv2\n",
    "\n",
    "\n",
    "# Step 2: Initialize matching model (ViT-Base)\n",
    "model = VideoMAEv2()\n",
    "model.head = torch.nn.Identity()  # Remove classification head for feature extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b63941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./converted/videomaev2-base-p16-16f-pretrain.pth\", map_location='cpu')\n",
    "\n",
    "if 'model' in checkpoint:\n",
    "    state_dict = checkpoint['model']\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255e37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# Preprocessing transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df645ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_frames(video_path, num_frames=16, stride=4):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "\n",
    "    clips = []\n",
    "    for i in range(0, len(frames) - num_frames + 1, stride):\n",
    "        clip = frames[i:i+num_frames]\n",
    "        if len(clip) == num_frames:\n",
    "            clips.append(clip)\n",
    "    return clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e2aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: S3_CofHoney_C1.mp4\n",
      "Processing: S2_Hotdog_C1.mp4\n",
      "Processing: S4_Tea_C1.mp4\n",
      "Processing: S4_CofHoney_C1.mp4\n",
      "Processing: S3_Hotdog_C1.mp4\n",
      "Processing: S2_Peanut_C1.mp4\n",
      "Processing: S3_Coffee_C1.mp4\n",
      "Processing: S1_Pealate_C1.mp4\n",
      "Processing: S1_CofHoney_C1.mp4\n",
      "Processing: S2_CofHoney_C1.mp4\n",
      "Processing: S2_Tea_C1.mp4\n",
      "Processing: S2_Pealate_C1.mp4\n",
      "Processing: S3_Cheese_C1.mp4\n",
      "Processing: S1_Coffee_C1.mp4\n",
      "Processing: S2_Coffee_C1.mp4\n",
      "Processing: S3_Tea_C1.mp4\n",
      "Processing: S2_Cheese_C1.mp4\n",
      "Processing: S4_Coffee_C1.mp4\n",
      "Processing: S3_Peanut_C1.mp4\n",
      "Processing: S4_Hotdog_C1.mp4\n",
      "Processing: S1_Cheese_C1.mp4\n",
      "Processing: S3_Pealate_C1.mp4\n",
      "Processing: S1_Tea_C1.mp4\n",
      "Processing: S4_Cheese_C1.mp4\n",
      "Processing: S1_Hotdog_C1.mp4\n",
      "Processing: S1_Peanut_C1.mp4\n",
      "Processing: S4_Peanut_C1.mp4\n",
      "Processing: S4_Pealate_C1.mp4\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def extract_features_from_clip(clip, model):\n",
    "    from PIL import Image\n",
    "    clip_tensor = torch.stack([transform(Image.fromarray(frame)) for frame in clip])  # [T, C, H, W]\n",
    "    clip_tensor = rearrange(clip_tensor, 't c h w -> 1 c t h w').to(device)           # [1, C, T, H, W]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = model.encoder.patch_embed(clip_tensor)  # [B, N, C]\n",
    "        _, N, _ = x.shape\n",
    "        mask = torch.zeros((1, N), dtype=torch.bool).to(device)\n",
    "\n",
    "        features = model.encoder(clip_tensor, mask)  # pass mask with correct length\n",
    "        features = features.mean(dim=1)  # global average pool\n",
    "\n",
    "    return features.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "# Process GTEA videos\n",
    "VIDEO_DIR = './gg/gtea/Videos'\n",
    "OUT_DIR = './features/gtea'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "for vid_file in os.listdir(VIDEO_DIR):\n",
    "    if not vid_file.endswith('.mp4'):\n",
    "        continue\n",
    "    print(f\"Processing: {vid_file}\")\n",
    "    video_path = os.path.join(VIDEO_DIR, vid_file)\n",
    "    clips = read_video_frames(video_path, num_frames=16, stride=1)\n",
    "\n",
    "    all_feats = []\n",
    "    for clip in clips:\n",
    "        feat = extract_features_from_clip(clip, model)\n",
    "        all_feats.append(feat)\n",
    "\n",
    "    all_feats = np.stack(all_feats)  \n",
    "    np.save(os.path.join(OUT_DIR, vid_file.replace('.mp4', '.npy')), all_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4658cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = np.stack(all_feats)  # Shape: [num_clips, feature_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff594b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: S3_CofHoney_C1.mp4\n",
      "Processing: S2_Hotdog_C1.mp4\n",
      "Processing: S4_Tea_C1.mp4\n",
      "Processing: S4_CofHoney_C1.mp4\n",
      "Processing: S3_Hotdog_C1.mp4\n",
      "Processing: S2_Peanut_C1.mp4\n",
      "Processing: S3_Coffee_C1.mp4\n",
      "Processing: S1_Pealate_C1.mp4\n",
      "Processing: S1_CofHoney_C1.mp4\n",
      "Processing: S2_CofHoney_C1.mp4\n",
      "Processing: S2_Tea_C1.mp4\n",
      "Processing: S2_Pealate_C1.mp4\n",
      "Processing: S3_Cheese_C1.mp4\n",
      "Processing: S1_Coffee_C1.mp4\n",
      "Processing: S2_Coffee_C1.mp4\n",
      "Processing: S3_Tea_C1.mp4\n",
      "Processing: S2_Cheese_C1.mp4\n",
      "Processing: S4_Coffee_C1.mp4\n",
      "Processing: S3_Peanut_C1.mp4\n",
      "Processing: S4_Hotdog_C1.mp4\n",
      "Processing: S1_Cheese_C1.mp4\n",
      "Processing: S3_Pealate_C1.mp4\n",
      "Processing: S1_Tea_C1.mp4\n",
      "Processing: S4_Cheese_C1.mp4\n",
      "Processing: S1_Hotdog_C1.mp4\n",
      "Processing: S1_Peanut_C1.mp4\n",
      "Processing: S4_Peanut_C1.mp4\n",
      "Processing: S4_Pealate_C1.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from torchvision import transforms\n",
    "from moviepy.editor import VideoFileClip\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def read_video_frames(video_path, num_frames=16, stride=1):\n",
    "    \"\"\"Reads overlapping clips of frames from a video file.\"\"\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    frames = [frame[:, :, ::-1] for frame in clip.iter_frames(fps=clip.fps)]\n",
    "    clip.reader.close()\n",
    "\n",
    "    if clip.audio is not None:\n",
    "        clip.audio.reader.close_proc()\n",
    "\n",
    "    total_frames = len(frames)\n",
    "    clips = []\n",
    "\n",
    "    for i in range(0, total_frames - num_frames + 1, stride):\n",
    "        clips.append(frames[i:i + num_frames])\n",
    "\n",
    "    return clips, total_frames\n",
    "\n",
    "\n",
    "def extract_features_from_clip(clip, model):\n",
    "    \"\"\"Extract one feature vector from a clip using VideoMAEv2.\"\"\"\n",
    "    clip_tensor = torch.stack([transform(Image.fromarray(frame)) for frame in clip])  # [T, C, H, W]\n",
    "    clip_tensor = rearrange(clip_tensor, 't c h w -> 1 c t h w').to(device)          # [1, C, T, H, W]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = model.encoder.patch_embed(clip_tensor)\n",
    "        _, N, _ = x.shape\n",
    "        mask = torch.zeros((1, N), dtype=torch.bool).to(device)\n",
    "\n",
    "        features = model.encoder(clip_tensor, mask)\n",
    "        features = features.mean(dim=1)  # global average pool\n",
    "\n",
    "    return features.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Paths\n",
    "VIDEO_DIR = '/home/cair/Dharmendra/gg/gtea/Videos'\n",
    "OUT_DIR = '/home/cair/Dharmendra/features/gtea2'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loop over videos\n",
    "for vid_file in os.listdir(VIDEO_DIR):\n",
    "    if not vid_file.endswith('.mp4'):\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing: {vid_file}\")\n",
    "    video_path = os.path.join(VIDEO_DIR, vid_file)\n",
    "\n",
    "    clips, total_frames = read_video_frames(video_path, num_frames=16, stride=1)\n",
    "\n",
    "    all_feats = []\n",
    "    for clip in clips:\n",
    "        feat = extract_features_from_clip(clip, model)\n",
    "        all_feats.append(feat)\n",
    "\n",
    "    all_feats = np.stack(all_feats)  # shape = [T', D]\n",
    "\n",
    "    # üî• Interpolate to match total number of video frames\n",
    "    interpolated_feats = zoom(all_feats, (total_frames / all_feats.shape[0], 1), order=1)  # [T, D]\n",
    "\n",
    "    out_path = os.path.join(OUT_DIR, vid_file.replace('.mp4', '.npy'))\n",
    "    np.save(out_path, interpolated_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31409180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   4%|‚ñé         | 1/28 [00:29<13:17, 29.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S3_CofHoney_C1.mp4: (110, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   7%|‚ñã         | 2/28 [00:54<11:43, 27.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S2_Hotdog_C1.mp4: (100, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  11%|‚ñà         | 3/28 [01:31<13:05, 31.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S4_Tea_C1.mp4: (142, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  14%|‚ñà‚ñç        | 4/28 [01:58<11:55, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S4_CofHoney_C1.mp4: (107, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  18%|‚ñà‚ñä        | 5/28 [02:25<11:02, 28.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S3_Hotdog_C1.mp4: (106, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  21%|‚ñà‚ñà‚ñè       | 6/28 [03:10<12:32, 34.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S2_Peanut_C1.mp4: (182, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  25%|‚ñà‚ñà‚ñå       | 7/28 [03:47<12:13, 34.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S3_Coffee_C1.mp4: (147, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  29%|‚ñà‚ñà‚ñä       | 8/28 [04:32<12:44, 38.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S1_Pealate_C1.mp4: (172, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  32%|‚ñà‚ñà‚ñà‚ñè      | 9/28 [05:12<12:20, 38.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S1_CofHoney_C1.mp4: (153, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  36%|‚ñà‚ñà‚ñà‚ñå      | 10/28 [05:37<10:24, 34.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S2_CofHoney_C1.mp4: (101, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  39%|‚ñà‚ñà‚ñà‚ñâ      | 11/28 [06:22<10:41, 37.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S2_Tea_C1.mp4: (175, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 12/28 [07:01<10:09, 38.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S2_Pealate_C1.mp4: (146, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 13/28 [07:31<08:54, 35.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S3_Cheese_C1.mp4: (113, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 14/28 [08:09<08:27, 36.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S1_Coffee_C1.mp4: (146, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 15/28 [09:07<09:18, 42.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S2_Coffee_C1.mp4: (225, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 16/28 [09:51<08:39, 43.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S3_Tea_C1.mp4: (169, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 17/28 [10:12<06:40, 36.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S2_Cheese_C1.mp4: (78, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 18/28 [10:42<05:45, 34.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S4_Coffee_C1.mp4: (119, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 19/28 [11:12<04:58, 33.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S3_Peanut_C1.mp4: (119, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 20/28 [11:33<03:56, 29.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S4_Hotdog_C1.mp4: (80, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 21/28 [12:04<03:29, 29.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S1_Cheese_C1.mp4: (116, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 22/28 [12:42<03:14, 32.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S3_Pealate_C1.mp4: (145, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 23/28 [13:46<03:29, 41.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S1_Tea_C1.mp4: (250, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 24/28 [14:11<02:27, 36.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S4_Cheese_C1.mp4: (99, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 25/28 [14:34<01:38, 32.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S1_Hotdog_C1.mp4: (88, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/28 [15:27<01:17, 38.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S1_Peanut_C1.mp4: (204, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 27/28 [15:58<00:36, 36.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S4_Peanut_C1.mp4: (115, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [16:36<00:00, 35.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed S4_Pealate_C1.mp4: (154, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import decord\n",
    "from decord import VideoReader, cpu\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "# Import VideoMAEv2 model\n",
    "from VideoMAEv2.models.modeling_pretrain import pretrain_videomae_base_patch16_224 as VideoMAEv2\n",
    "\n",
    "# -----------------------------\n",
    "# Configurations\n",
    "# -----------------------------\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "VIDEO_DIR = '/home/cair/Dharmendra/gg/gtea/Videos'\n",
    "OUT_DIR = '/home/cair/Dharmendra/features/videomaev2_temporal2'\n",
    "CHECKPOINT_PATH = './converted/videomaev2-base-p16-16f-pretrain.pth'\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Temporal Encoder (Conv1D)\n",
    "# -----------------------------\n",
    "class TemporalConvEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=1536, hidden_dim=512, out_dim=768, kernel_size=3):\n",
    "        super(TemporalConvEncoder, self).__init__()\n",
    "        self.temporal_encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, out_dim, kernel_size, padding=kernel_size//2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: [T, D]\n",
    "        x = x.transpose(0, 1).unsqueeze(0)   # [1, D, T]\n",
    "        x = self.temporal_encoder(x)\n",
    "        x = x.squeeze(0).transpose(0, 1)     # [T, D]\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Load VideoMAEv2 Pretrained Model\n",
    "# -----------------------------\n",
    "def load_videomae_encoder():\n",
    "    model = VideoMAEv2()\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "\n",
    "    if 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Feature Extraction Pipeline\n",
    "# -----------------------------\n",
    "def extract_features_from_video(model, video_path, clip_len=16, stride=8):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    total_frames = len(vr)\n",
    "    all_feats = []\n",
    "\n",
    "    transform = Compose([\n",
    "        Resize(256),\n",
    "        CenterCrop(224),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    for start_idx in range(0, total_frames - clip_len + 1, stride):\n",
    "        try:\n",
    "            # Get clip frames\n",
    "            frame_indices = list(range(start_idx, start_idx + clip_len))\n",
    "            clip = vr.get_batch(frame_indices).asnumpy()\n",
    "            \n",
    "            # Process each frame\n",
    "            processed_frames = []\n",
    "            for frame in clip:\n",
    "                pil_frame = Image.fromarray(frame)\n",
    "                transformed_frame = transform(pil_frame)\n",
    "                processed_frames.append(transformed_frame)\n",
    "            \n",
    "            # Prepare clip tensor [1, C, T, H, W]\n",
    "            clip = torch.stack(processed_frames)  # [T, C, H, W]\n",
    "            clip = clip.unsqueeze(0).to(DEVICE)  # [1, T, C, H, W]\n",
    "            clip = clip.permute(0, 2, 1, 3, 4)   # [1, C, T, H, W]\n",
    "\n",
    "            # Try different mask approaches\n",
    "            try:\n",
    "                # First try: no mask\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(clip)\n",
    "            except TypeError:\n",
    "                try:\n",
    "                    # Second try: mask with 1568 patches\n",
    "                    mask = torch.zeros((1, 1568), dtype=torch.bool).to(DEVICE)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(clip, mask)\n",
    "                except RuntimeError:\n",
    "                    # Third try: mask with 3136 patches\n",
    "                    mask = torch.zeros((1, 3136), dtype=torch.bool).to(DEVICE)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(clip, mask)\n",
    "            \n",
    "            # Handle model outputs\n",
    "            if isinstance(outputs, tuple):\n",
    "                features = outputs[0]  # Assume first element is features\n",
    "            else:\n",
    "                features = outputs\n",
    "            \n",
    "            # Extract features (using CLS token or average pooling)\n",
    "            if features.dim() == 3:  # [batch, seq_len, dim]\n",
    "                feat = features[:, 0]  # CLS token\n",
    "            elif features.dim() == 4:  # [batch, channels, height, width]\n",
    "                feat = features.mean(dim=[2, 3])  # Global average pooling\n",
    "            else:\n",
    "                feat = features.mean(dim=1)  # Average pooling\n",
    "            \n",
    "            all_feats.append(feat.squeeze(0).cpu())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing clip starting at frame {start_idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(all_feats) == 0:\n",
    "        return None\n",
    "\n",
    "    return torch.stack(all_feats)  # [T, D]\n",
    "\n",
    "# -----------------------------\n",
    "# Main Processing Loop\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # Initialize models\n",
    "    model = load_videomae_encoder()\n",
    "    temporal_encoder = TemporalConvEncoder().to(DEVICE)\n",
    "    temporal_encoder.eval()\n",
    "\n",
    "    # Get video files\n",
    "    video_files = [f for f in os.listdir(VIDEO_DIR) if f.endswith(('.mp4', '.avi'))]\n",
    "    \n",
    "    # Process each video\n",
    "    for vid_file in tqdm(video_files, desc=\"Extracting Features\"):\n",
    "        vid_path = os.path.join(VIDEO_DIR, vid_file)\n",
    "        output_path = os.path.join(OUT_DIR, vid_file.replace('.mp4', '.npy').replace('.avi', '.npy'))\n",
    "\n",
    "        try:\n",
    "            # Extract raw features\n",
    "            raw_feats = extract_features_from_video(model, vid_path)\n",
    "            \n",
    "            if raw_feats is not None:\n",
    "                # Apply temporal encoding\n",
    "                with torch.no_grad():\n",
    "                    feats = temporal_encoder(raw_feats.to(DEVICE)).cpu().numpy()\n",
    "                \n",
    "                # Save features\n",
    "                np.save(output_path, feats)\n",
    "                print(f\"Successfully processed {vid_file}: {feats.shape}\")\n",
    "            else:\n",
    "                print(f\"Skipping {vid_file} due to processing errors\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {vid_file}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a6cb38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping S3_CofHoney_C1.mp4, already extracted.\n",
      "Skipping S2_Hotdog_C1.mp4, already extracted.\n",
      "Skipping S4_Tea_C1.mp4, already extracted.\n",
      "Skipping S4_CofHoney_C1.mp4, already extracted.\n",
      "Skipping S3_Hotdog_C1.mp4, already extracted.\n",
      "Skipping S2_Peanut_C1.mp4, already extracted.\n",
      "Skipping S3_Coffee_C1.mp4, already extracted.\n",
      "Skipping S1_Pealate_C1.mp4, already extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  32%|‚ñà‚ñà‚ñà‚ñè      | 9/28 [05:15<11:06, 35.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S1_CofHoney_C1.mp4 -> (1220, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  36%|‚ñà‚ñà‚ñà‚ñå      | 10/28 [08:32<17:11, 57.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S2_CofHoney_C1.mp4 -> (808, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  39%|‚ñà‚ñà‚ñà‚ñâ      | 11/28 [14:22<29:48, 105.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S2_Tea_C1.mp4 -> (1397, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 12/28 [28:53<1:06:43, 250.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S2_Pealate_C1.mp4 -> (1166, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 13/28 [36:58<1:15:05, 300.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S3_Cheese_C1.mp4 -> (898, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 14/28 [41:59<1:10:06, 300.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S1_Coffee_C1.mp4 -> (1163, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 15/28 [49:39<1:13:42, 340.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S2_Coffee_C1.mp4 -> (1799, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 16/28 [55:25<1:08:22, 341.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S3_Tea_C1.mp4 -> (1346, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 17/28 [58:06<53:35, 292.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S2_Cheese_C1.mp4 -> (619, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 18/28 [1:02:06<46:16, 277.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S4_Coffee_C1.mp4 -> (949, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 19/28 [1:06:43<41:38, 277.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S3_Peanut_C1.mp4 -> (949, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 20/28 [1:09:55<33:41, 252.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S4_Hotdog_C1.mp4 -> (640, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 21/28 [1:16:21<34:03, 291.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S1_Cheese_C1.mp4 -> (928, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 22/28 [1:25:42<37:07, 371.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S3_Pealate_C1.mp4 -> (1154, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 23/28 [1:36:44<38:07, 457.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S1_Tea_C1.mp4 -> (1994, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 24/28 [1:40:22<25:44, 386.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S4_Cheese_C1.mp4 -> (790, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 25/28 [1:43:50<16:39, 333.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S1_Hotdog_C1.mp4 -> (703, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/28 [1:51:14<12:12, 366.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S1_Peanut_C1.mp4 -> (1628, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 27/28 [1:55:06<05:26, 326.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S4_Peanut_C1.mp4 -> (919, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [1:59:48<00:00, 256.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S4_Pealate_C1.mp4 -> (1229, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from decord import VideoReader, cpu\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import your pretrained model (make sure this import works)\n",
    "from VideoMAEv2.models.modeling_pretrain import pretrain_videomae_base_patch16_224 as VideoMAEv2\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "VIDEO_DIR = '/home/cair/Dharmendra/gg/gtea/Videos'  # Change as needed\n",
    "OUT_DIR = '/home/cair/Dharmendra/features/videomaev2_temporal3'\n",
    "CHECKPOINT_PATH = './converted/videomaev2-base-p16-16f-pretrain.pth'  # Change as needed\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Temporal conv encoder for smoothing features (optional)\n",
    "class TemporalConvEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=512, out_dim=768, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.temporal_encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hidden_dim, kernel_size, padding=kernel_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, out_dim, kernel_size, padding=kernel_size // 2),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x shape: [T, D]\n",
    "        x = x.transpose(0, 1).unsqueeze(0)   # [1, D, T]\n",
    "        x = self.temporal_encoder(x)\n",
    "        x = x.squeeze(0).transpose(0, 1)     # [T, D]\n",
    "        return x\n",
    "\n",
    "def load_videomae_encoder():\n",
    "    model = VideoMAEv2()\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "    state_dict = checkpoint.get('model', checkpoint)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def extract_features_from_video(model, temporal_encoder, video_path, clip_len=16, stride=1):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    total_frames = len(vr)\n",
    "    all_feats = []\n",
    "\n",
    "    transform = Compose([\n",
    "        Resize(256),\n",
    "        CenterCrop(224),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                  std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    for start_idx in range(0, total_frames - clip_len + 1, stride):\n",
    "        clip = vr.get_batch(list(range(start_idx, start_idx + clip_len))).asnumpy()\n",
    "        processed_frames = []\n",
    "        for frame in clip:\n",
    "            pil_frame = Image.fromarray(frame)\n",
    "            processed_frames.append(transform(pil_frame))\n",
    "\n",
    "        clip_tensor = torch.stack(processed_frames)  # [T, C, H, W]\n",
    "        clip_tensor = clip_tensor.unsqueeze(0).to(DEVICE)  # [1, T, C, H, W]\n",
    "        clip_tensor = clip_tensor.permute(0, 2, 1, 3, 4)   # [1, C, T, H, W]\n",
    "\n",
    "        mask = torch.zeros((clip_tensor.shape[0], model.encoder.patch_embed.num_patches), dtype=torch.bool).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latent = model(clip_tensor, mask)  # [B, N_masked, dim]\n",
    "\n",
    "        # latent shape depends on model, get features for CLS token or average\n",
    "        if latent.dim() == 3:  # [B, seq_len, dim]\n",
    "            feat = latent[:, 0, :]  # CLS token feature\n",
    "        else:\n",
    "            feat = latent.mean(dim=1)\n",
    "\n",
    "        all_feats.append(feat.squeeze(0).cpu())\n",
    "\n",
    "    if len(all_feats) == 0:\n",
    "        return None\n",
    "\n",
    "    feats = torch.stack(all_feats)  # [T, D]\n",
    "\n",
    "    # Optionally smooth features with temporal encoder\n",
    "    with torch.no_grad():\n",
    "        feats = temporal_encoder(feats.to(DEVICE)).cpu()\n",
    "\n",
    "    return feats.numpy()\n",
    "\n",
    "def main():\n",
    "    model = load_videomae_encoder()\n",
    "    temporal_encoder = TemporalConvEncoder(in_dim=1536, out_dim=768).to(DEVICE)\n",
    "    temporal_encoder.eval()\n",
    "\n",
    "    video_files = [f for f in os.listdir(VIDEO_DIR) if f.endswith('.mp4') or f.endswith('.avi')]\n",
    "\n",
    "    for vid_file in tqdm(video_files, desc=\"Extracting Features\"):\n",
    "        vid_path = os.path.join(VIDEO_DIR, vid_file)\n",
    "        \n",
    "        output_path = os.path.join(OUT_DIR, vid_file.rsplit('.',1)[0] + '.npy')\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Skipping {vid_file}, already extracted.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            feats = extract_features_from_video(model, temporal_encoder, vid_path)\n",
    "            if feats is not None:\n",
    "                np.save(output_path, feats)\n",
    "                print(f\"Processed {vid_file} -> {feats.shape}\")\n",
    "            else:\n",
    "                print(f\"Skipping {vid_file}, no features extracted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {vid_file}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0f44a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videomaev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
