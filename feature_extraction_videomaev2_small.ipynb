{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc378e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully converted Hugging Face .bin to videomaev2.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from VideoMAEv2.models.modeling_pretrain import pretrain_videomae_base_patch16_224 as VideoMAEv2\n",
    "\n",
    "# Initialize model\n",
    "model = VideoMAEv2()\n",
    "model.head = torch.nn.Identity()  # Remove classification head for feature extraction\n",
    "\n",
    "# Load state_dict directly\n",
    "state_dict = torch.load(\"pytorch_model_l.bin\", map_location=\"cpu\")  # No [\"model\"] here!\n",
    "\n",
    "# Load weights into model\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Save as .pth\n",
    "torch.save(model.state_dict(), \"videomaev2.pth\")\n",
    "print(\"âœ… Successfully converted Hugging Face .bin to videomaev2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7891de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Loading VideoMAE model from vit_s_k710_dl_from_giant.pth\n",
      "âœ… Model loaded from checkpoint\n",
      "âœ… Model loaded successfully\n",
      "Found 28 videos to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:   4%|â–Ž         | 1/28 [01:03<28:44, 63.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 928 features to features/gteasmall/S1_Cheese_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:   7%|â–‹         | 2/28 [02:28<32:57, 76.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1220 features to features/gteasmall/S1_CofHoney_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  11%|â–ˆ         | 3/28 [03:54<33:33, 80.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1163 features to features/gteasmall/S1_Coffee_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  14%|â–ˆâ–        | 4/28 [04:46<27:39, 69.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 703 features to features/gteasmall/S1_Hotdog_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  18%|â–ˆâ–Š        | 5/28 [06:27<31:00, 80.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1369 features to features/gteasmall/S1_Pealate_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  21%|â–ˆâ–ˆâ–       | 6/28 [08:29<34:42, 94.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1628 features to features/gteasmall/S1_Peanut_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  25%|â–ˆâ–ˆâ–Œ       | 7/28 [10:56<39:08, 111.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1994 features to features/gteasmall/S1_Tea_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  29%|â–ˆâ–ˆâ–Š       | 8/28 [11:42<30:16, 90.84s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 619 features to features/gteasmall/S2_Cheese_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  32%|â–ˆâ–ˆâ–ˆâ–      | 9/28 [12:42<25:44, 81.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 808 features to features/gteasmall/S2_CofHoney_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 10/28 [14:56<29:13, 97.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1799 features to features/gteasmall/S2_Coffee_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 11/28 [15:55<24:18, 85.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 796 features to features/gteasmall/S2_Hotdog_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 12/28 [17:21<22:55, 85.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1166 features to features/gteasmall/S2_Pealate_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 13/28 [19:09<23:06, 92.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1450 features to features/gteasmall/S2_Peanut_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 14/28 [20:52<22:20, 95.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1397 features to features/gteasmall/S2_Tea_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 15/28 [21:59<18:50, 86.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 898 features to features/gteasmall/S3_Cheese_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 16/28 [23:04<16:04, 80.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 877 features to features/gteasmall/S3_CofHoney_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 17/28 [24:31<15:08, 82.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1175 features to features/gteasmall/S3_Coffee_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/28 [25:35<12:48, 76.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 847 features to features/gteasmall/S3_Hotdog_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 19/28 [27:03<12:01, 80.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1154 features to features/gteasmall/S3_Pealate_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/28 [28:33<11:05, 83.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 949 features to features/gteasmall/S3_Peanut_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 21/28 [30:33<10:58, 94.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1346 features to features/gteasmall/S3_Tea_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 22/28 [31:31<08:20, 83.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 790 features to features/gteasmall/S4_Cheese_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 23/28 [32:34<06:26, 77.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 856 features to features/gteasmall/S4_CofHoney_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 24/28 [33:46<05:02, 75.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 949 features to features/gteasmall/S4_Coffee_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 25/28 [34:34<03:22, 67.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 640 features to features/gteasmall/S4_Hotdog_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 26/28 [36:07<02:30, 75.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1229 features to features/gteasmall/S4_Pealate_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 27/28 [37:17<01:13, 73.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 919 features to features/gteasmall/S4_Peanut_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [38:43<00:00, 82.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1136 features to features/gteasmall/S4_Tea_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from VideoMAEv2.models.modeling_pretrain import pretrain_videomae_small_patch16_224 as VideoMAEv2\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_frames = 16\n",
    "stride = 1\n",
    "data_path = \"gg/gtea/Videos\"\n",
    "save_path = \"features/gteasmall\"\n",
    "ckpt_path = \"vit_s_k710_dl_from_giant.pth\"\n",
    "\n",
    "# Video preprocessing transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    \"\"\"Preprocess frames for VideoMAE input\"\"\"\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        # Apply transforms\n",
    "        frame_tensor = transform(frame)\n",
    "        processed_frames.append(frame_tensor)\n",
    "    \n",
    "    # Stack frames: [T, C, H, W]\n",
    "    video_tensor = torch.stack(processed_frames)\n",
    "    # Rearrange to [C, T, H, W] for VideoMAE\n",
    "    video_tensor = rearrange(video_tensor, 't c h w -> c t h w')\n",
    "    # Add batch dimension: [1, C, T, H, W]\n",
    "    video_tensor = video_tensor.unsqueeze(0)\n",
    "    \n",
    "    return video_tensor\n",
    "\n",
    "def compute_num_patches(video_tensor):\n",
    "    # Video tensor shape: [B, C, T, H, W]\n",
    "    _, _, T, H, W = video_tensor.shape\n",
    "    patch_size = 16\n",
    "    tubelet_size = 2\n",
    "    num_patches_per_frame = (H // patch_size) * (W // patch_size)\n",
    "    num_temporal_patches = T // tubelet_size\n",
    "    return num_temporal_patches * num_patches_per_frame\n",
    "\n",
    "def extract_features_from_video(video_path, model):\n",
    "    frames = read_video_frames(video_path)\n",
    "    if len(frames) < num_frames:\n",
    "        print(f\"âš ï¸ Skipping {video_path}: only {len(frames)} frames\")\n",
    "        return None\n",
    "\n",
    "    features = []\n",
    "    for start in range(0, len(frames) - num_frames + 1, stride):\n",
    "        clip = frames[start:start + num_frames]\n",
    "        \n",
    "        # Preprocess the clip\n",
    "        video_tensor = preprocess_frames(clip).to(device)  # [1, C, T, H, W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_size = video_tensor.shape[0]\n",
    "            num_patches = compute_num_patches(video_tensor)\n",
    "            # Create all-zero mask for no masking during inference\n",
    "            mask = torch.zeros(batch_size, num_patches, dtype=torch.bool, device=device)\n",
    "\n",
    "            # Extract features using VideoMAE encoder\n",
    "            outputs = model(video_tensor, mask)\n",
    "\n",
    "            # If outputs is a tuple, take the first element (features)\n",
    "            if isinstance(outputs, tuple):\n",
    "                feat = outputs[0]\n",
    "            else:\n",
    "                feat = outputs\n",
    "            \n",
    "            # Global average pooling over spatial and temporal dims\n",
    "            if feat.dim() == 5:  # [B, C, T, H, W]\n",
    "                feat = feat.mean(dim=[2, 3, 4])  # [B, C]\n",
    "            elif feat.dim() == 4:  # [B, T, H, W] or [B, C, H, W]\n",
    "                feat = feat.mean(dim=[2, 3])  # [B, T] or [B, C]\n",
    "            elif feat.dim() == 3:  # [B, T, D]\n",
    "                feat = feat.mean(dim=1)  # [B, D]\n",
    "\n",
    "            # Normalize features\n",
    "            feat = torch.nn.functional.normalize(feat, dim=-1)\n",
    "            features.append(feat.squeeze(0).cpu().numpy())\n",
    "\n",
    "    return np.stack(features) if features else None\n",
    "\n",
    "def main():\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    print(f\"ðŸ”§ Loading VideoMAE model from {ckpt_path}\")\n",
    "    try:\n",
    "        # Initialize model\n",
    "        model = VideoMAEv2()\n",
    "        model.head = torch.nn.Identity()  # Remove classification head for feature extraction\n",
    "        \n",
    "        # Load checkpoint\n",
    "        if os.path.exists(ckpt_path):\n",
    "            state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            print(\"âœ… Model loaded from checkpoint\")\n",
    "        else:\n",
    "            print(f\"âŒ Checkpoint not found: {ckpt_path}\")\n",
    "            return\n",
    "        \n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(\"âœ… Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"âŒ Data path does not exist: {data_path}\")\n",
    "        return\n",
    "\n",
    "    video_list = sorted([f for f in os.listdir(data_path) if f.endswith(('.mp4', '.avi', '.mov'))])\n",
    "    \n",
    "    if not video_list:\n",
    "        print(f\"No video files found in {data_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_list)} videos to process\")\n",
    "\n",
    "    for video_name in tqdm(video_list, desc=\"Extracting features\"):\n",
    "        video_path = os.path.join(data_path, video_name)\n",
    "        save_file = os.path.join(save_path, f\"{os.path.splitext(video_name)[0]}.npy\")\n",
    "\n",
    "        if os.path.exists(save_file):\n",
    "            print(f\" Skipping {video_name}: features already exist\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            features = extract_features_from_video(video_path, model)\n",
    "            if features is not None:\n",
    "                np.save(save_file, features.astype(np.float32))\n",
    "                print(f\" Saved {features.shape[0]} features to {save_file}\")\n",
    "            else:\n",
    "                print(f\"Failed to extract features from {video_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {video_name}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "907a8ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cair/miniconda3/envs/videomaev2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from videomaev2.pth\n",
      "Model loaded\n",
      "Found 28 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<00:00, 68759.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from VideoMAEv2.models.modeling_pretrain import pretrain_videomae_base_patch16_224 as VideoMAEv2\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_frames = 16\n",
    "stride = 1\n",
    "data_path = \"gg/gtea/Videos\"\n",
    "save_path = \"features/gteam\"\n",
    "ckpt_path = \"videomaev2.pth\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    processed = [transform(f) for f in frames]\n",
    "    video_tensor = torch.stack(processed)  # [T, C, H, W]\n",
    "    video_tensor = rearrange(video_tensor, 't c h w -> c t h w').unsqueeze(0)  # [1, C, T, H, W]\n",
    "    return video_tensor\n",
    "\n",
    "def compute_num_patches(video_tensor):\n",
    "    _, _, T, H, W = video_tensor.shape\n",
    "    patch_size = 16\n",
    "    tubelet_size = 2\n",
    "    num_patches_per_frame = (H // patch_size) * (W // patch_size)\n",
    "    return (T // tubelet_size) * num_patches_per_frame\n",
    "\n",
    "def extract_features_from_video(video_path, model):\n",
    "    frames = read_video_frames(video_path)\n",
    "    if len(frames) < num_frames:\n",
    "        print(f\"Skipping {video_path}: only {len(frames)} frames\")\n",
    "        return None\n",
    "\n",
    "    features = []\n",
    "    for start in range(0, len(frames) - num_frames + 1, stride):\n",
    "        clip = frames[start:start + num_frames]\n",
    "        video_tensor = preprocess_frames(clip).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_size = video_tensor.shape[0]\n",
    "            num_patches = compute_num_patches(video_tensor)\n",
    "            mask = torch.zeros(batch_size, num_patches, dtype=torch.bool, device=device)\n",
    "            outputs = model(video_tensor, mask)\n",
    "\n",
    "            feat = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "\n",
    "            if feat.dim() == 5:\n",
    "                feat = feat.mean(dim=[2, 3, 4])\n",
    "            elif feat.dim() == 4:\n",
    "                feat = feat.mean(dim=[2, 3])\n",
    "            elif feat.dim() == 3:\n",
    "                feat = feat.mean(dim=1)\n",
    "\n",
    "            feat = torch.nn.functional.normalize(feat, dim=-1)\n",
    "            features.append(feat.squeeze(0).cpu().numpy())\n",
    "\n",
    "    return np.stack(features) if features else None\n",
    "\n",
    "def main():\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading model from {ckpt_path}\")\n",
    "    model = VideoMAEv2()\n",
    "    model.head = torch.nn.Identity()\n",
    "    if os.path.exists(ckpt_path):\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"Model loaded\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found: {ckpt_path}\")\n",
    "        return\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data path does not exist: {data_path}\")\n",
    "        return\n",
    "\n",
    "    videos = sorted([f for f in os.listdir(data_path) if f.endswith(('.mp4', '.avi', '.mov'))])\n",
    "    if not videos:\n",
    "        print(f\"No videos found in {data_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(videos)} videos\")\n",
    "\n",
    "    for video_name in tqdm(videos, desc=\"Extracting features\"):\n",
    "        save_file = os.path.join(save_path, f\"{os.path.splitext(video_name)[0]}.npy\")\n",
    "        if os.path.exists(save_file):\n",
    "            continue\n",
    "        try:\n",
    "            features = extract_features_from_video(os.path.join(data_path, video_name), model)\n",
    "            if features is not None:\n",
    "                np.save(save_file, features.astype(np.float32))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_name}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91adc401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videomaev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
