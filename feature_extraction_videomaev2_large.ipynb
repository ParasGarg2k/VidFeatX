{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc378e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully converted Hugging Face .bin to videomaev2.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from VideoMAEv2.models.modeling_pretrain import pretrain_videomae_base_patch16_224 as VideoMAEv2\n",
    "\n",
    "# Initialize model\n",
    "model = VideoMAEv2()\n",
    "model.head = torch.nn.Identity()  # Remove classification head for feature extraction\n",
    "\n",
    "# Load state_dict directly\n",
    "state_dict = torch.load(\"pytorch_model_l.bin\", map_location=\"cpu\")  # No [\"model\"] here!\n",
    "\n",
    "# Load weights into model\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Save as .pth\n",
    "torch.save(model.state_dict(), \"videomaev2.pth\")\n",
    "print(\"‚úÖ Successfully converted Hugging Face .bin to videomaev2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7891de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading VideoMAE model from videomaev2.pth\n",
      "‚úÖ Model loaded from checkpoint\n",
      "‚úÖ Model loaded successfully\n",
      "üìÅ Found 28 videos to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:   4%|‚ñé         | 1/28 [02:46<1:14:58, 166.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 928 features to features/gteam/S1_Cheese_C1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:   4%|‚ñé         | 1/28 [03:47<1:42:34, 227.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 161\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Error processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 151\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m         np\u001b[38;5;241m.\u001b[39msave(save_file, features\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "Cell \u001b[0;32mIn[15], line 83\u001b[0m, in \u001b[0;36mextract_features_from_video\u001b[0;34m(video_path, model)\u001b[0m\n\u001b[1;32m     80\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, num_patches, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Extract features using VideoMAE encoder\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# If outputs is a tuple, take the first element (features)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/videomaev2/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Dharmendra/VideoMAEv2/models/modeling_pretrain.py:349\u001b[0m, in \u001b[0;36mPretrainVisionTransformer.forward\u001b[0;34m(self, x, mask, decode_mask)\u001b[0m\n\u001b[1;32m    345\u001b[0m B, N_vis, C \u001b[38;5;241m=\u001b[39m x_vis\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# we don't unshuffle the correct visible token order,\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# but shuffle the pos embedding accorddingly.\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m expand_pos_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    350\u001b[0m     x\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    351\u001b[0m pos_emd_vis \u001b[38;5;241m=\u001b[39m expand_pos_embed[\u001b[38;5;241m~\u001b[39mmask]\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C)\n\u001b[1;32m    352\u001b[0m pos_emd_mask \u001b[38;5;241m=\u001b[39m expand_pos_embed[decode_vis]\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from VideoMAEv2.models.modeling_pretrain import pretrain_videomae_base_patch16_224 as VideoMAEv2\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_frames = 16\n",
    "stride = 1\n",
    "data_path = \"gg/gtea/Videos\"\n",
    "save_path = \"features/gteam\"\n",
    "ckpt_path = \"videomaev2.pth\"\n",
    "\n",
    "# Video preprocessing transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    \"\"\"Preprocess frames for VideoMAE input\"\"\"\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        # Apply transforms\n",
    "        frame_tensor = transform(frame)\n",
    "        processed_frames.append(frame_tensor)\n",
    "    \n",
    "    # Stack frames: [T, C, H, W]\n",
    "    video_tensor = torch.stack(processed_frames)\n",
    "    # Rearrange to [C, T, H, W] for VideoMAE\n",
    "    video_tensor = rearrange(video_tensor, 't c h w -> c t h w')\n",
    "    # Add batch dimension: [1, C, T, H, W]\n",
    "    video_tensor = video_tensor.unsqueeze(0)\n",
    "    \n",
    "    return video_tensor\n",
    "\n",
    "def compute_num_patches(video_tensor):\n",
    "    # Video tensor shape: [B, C, T, H, W]\n",
    "    _, _, T, H, W = video_tensor.shape\n",
    "    patch_size = 16\n",
    "    tubelet_size = 2\n",
    "    num_patches_per_frame = (H // patch_size) * (W // patch_size)\n",
    "    num_temporal_patches = T // tubelet_size\n",
    "    return num_temporal_patches * num_patches_per_frame\n",
    "\n",
    "def extract_features_from_video(video_path, model):\n",
    "    frames = read_video_frames(video_path)\n",
    "    if len(frames) < num_frames:\n",
    "        print(f\"‚ö†Ô∏è Skipping {video_path}: only {len(frames)} frames\")\n",
    "        return None\n",
    "\n",
    "    features = []\n",
    "    for start in range(0, len(frames) - num_frames + 1, stride):\n",
    "        clip = frames[start:start + num_frames]\n",
    "        \n",
    "        # Preprocess the clip\n",
    "        video_tensor = preprocess_frames(clip).to(device)  # [1, C, T, H, W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_size = video_tensor.shape[0]\n",
    "            num_patches = compute_num_patches(video_tensor)\n",
    "            # Create all-zero mask for no masking during inference\n",
    "            mask = torch.zeros(batch_size, num_patches, dtype=torch.bool, device=device)\n",
    "\n",
    "            # Extract features using VideoMAE encoder\n",
    "            outputs = model(video_tensor, mask)\n",
    "\n",
    "            # If outputs is a tuple, take the first element (features)\n",
    "            if isinstance(outputs, tuple):\n",
    "                feat = outputs[0]\n",
    "            else:\n",
    "                feat = outputs\n",
    "            \n",
    "            # Global average pooling over spatial and temporal dims\n",
    "            if feat.dim() == 5:  # [B, C, T, H, W]\n",
    "                feat = feat.mean(dim=[2, 3, 4])  # [B, C]\n",
    "            elif feat.dim() == 4:  # [B, T, H, W] or [B, C, H, W]\n",
    "                feat = feat.mean(dim=[2, 3])  # [B, T] or [B, C]\n",
    "            elif feat.dim() == 3:  # [B, T, D]\n",
    "                feat = feat.mean(dim=1)  # [B, D]\n",
    "\n",
    "            # Normalize features\n",
    "            feat = torch.nn.functional.normalize(feat, dim=-1)\n",
    "            features.append(feat.squeeze(0).cpu().numpy())\n",
    "\n",
    "    return np.stack(features) if features else None\n",
    "\n",
    "def main():\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    print(f\"üîß Loading VideoMAE model from {ckpt_path}\")\n",
    "    try:\n",
    "        # Initialize model\n",
    "        model = VideoMAEv2()\n",
    "        model.head = torch.nn.Identity()  # Remove classification head for feature extraction\n",
    "        \n",
    "        # Load checkpoint\n",
    "        if os.path.exists(ckpt_path):\n",
    "            state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            print(\"‚úÖ Model loaded from checkpoint\")\n",
    "        else:\n",
    "            print(f\"‚ùå Checkpoint not found: {ckpt_path}\")\n",
    "            return\n",
    "        \n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(\"‚úÖ Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path does not exist: {data_path}\")\n",
    "        return\n",
    "\n",
    "    video_list = sorted([f for f in os.listdir(data_path) if f.endswith(('.mp4', '.avi', '.mov'))])\n",
    "    \n",
    "    if not video_list:\n",
    "        print(f\"No video files found in {data_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_list)} videos to process\")\n",
    "\n",
    "    for video_name in tqdm(video_list, desc=\"Extracting features\"):\n",
    "        video_path = os.path.join(data_path, video_name)\n",
    "        save_file = os.path.join(save_path, f\"{os.path.splitext(video_name)[0]}.npy\")\n",
    "\n",
    "        if os.path.exists(save_file):\n",
    "            print(f\" Skipping {video_name}: features already exist\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            features = extract_features_from_video(video_path, model)\n",
    "            if features is not None:\n",
    "                np.save(save_file, features.astype(np.float32))\n",
    "                print(f\" Saved {features.shape[0]} features to {save_file}\")\n",
    "            else:\n",
    "                print(f\"Failed to extract features from {video_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {video_name}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "907a8ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cair/miniconda3/envs/videomaev2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from videomaev2.pth\n",
      "Model loaded\n",
      "Found 28 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 68759.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from VideoMAEv2.models.modeling_pretrain import pretrain_videomae_base_patch16_224 as VideoMAEv2\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_frames = 16\n",
    "stride = 1\n",
    "data_path = \"gg/gtea/Videos\"\n",
    "save_path = \"features/gteam\"\n",
    "ckpt_path = \"videomaev2.pth\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    processed = [transform(f) for f in frames]\n",
    "    video_tensor = torch.stack(processed)  # [T, C, H, W]\n",
    "    video_tensor = rearrange(video_tensor, 't c h w -> c t h w').unsqueeze(0)  # [1, C, T, H, W]\n",
    "    return video_tensor\n",
    "\n",
    "def compute_num_patches(video_tensor):\n",
    "    _, _, T, H, W = video_tensor.shape\n",
    "    patch_size = 16\n",
    "    tubelet_size = 2\n",
    "    num_patches_per_frame = (H // patch_size) * (W // patch_size)\n",
    "    return (T // tubelet_size) * num_patches_per_frame\n",
    "\n",
    "def extract_features_from_video(video_path, model):\n",
    "    frames = read_video_frames(video_path)\n",
    "    if len(frames) < num_frames:\n",
    "        print(f\"Skipping {video_path}: only {len(frames)} frames\")\n",
    "        return None\n",
    "\n",
    "    features = []\n",
    "    for start in range(0, len(frames) - num_frames + 1, stride):\n",
    "        clip = frames[start:start + num_frames]\n",
    "        video_tensor = preprocess_frames(clip).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_size = video_tensor.shape[0]\n",
    "            num_patches = compute_num_patches(video_tensor)\n",
    "            mask = torch.zeros(batch_size, num_patches, dtype=torch.bool, device=device)\n",
    "            outputs = model(video_tensor, mask)\n",
    "\n",
    "            feat = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "\n",
    "            if feat.dim() == 5:\n",
    "                feat = feat.mean(dim=[2, 3, 4])\n",
    "            elif feat.dim() == 4:\n",
    "                feat = feat.mean(dim=[2, 3])\n",
    "            elif feat.dim() == 3:\n",
    "                feat = feat.mean(dim=1)\n",
    "\n",
    "            feat = torch.nn.functional.normalize(feat, dim=-1)\n",
    "            features.append(feat.squeeze(0).cpu().numpy())\n",
    "\n",
    "    return np.stack(features) if features else None\n",
    "\n",
    "def main():\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading model from {ckpt_path}\")\n",
    "    model = VideoMAEv2()\n",
    "    model.head = torch.nn.Identity()\n",
    "    if os.path.exists(ckpt_path):\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"Model loaded\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found: {ckpt_path}\")\n",
    "        return\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data path does not exist: {data_path}\")\n",
    "        return\n",
    "\n",
    "    videos = sorted([f for f in os.listdir(data_path) if f.endswith(('.mp4', '.avi', '.mov'))])\n",
    "    if not videos:\n",
    "        print(f\"No videos found in {data_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(videos)} videos\")\n",
    "\n",
    "    for video_name in tqdm(videos, desc=\"Extracting features\"):\n",
    "        save_file = os.path.join(save_path, f\"{os.path.splitext(video_name)[0]}.npy\")\n",
    "        if os.path.exists(save_file):\n",
    "            continue\n",
    "        try:\n",
    "            features = extract_features_from_video(os.path.join(data_path, video_name), model)\n",
    "            if features is not None:\n",
    "                np.save(save_file, features.astype(np.float32))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_name}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91adc401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videomaev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
